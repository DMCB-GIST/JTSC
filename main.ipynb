{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import random\n",
    "import pickle\n",
    "import warnings \n",
    "warnings.filterwarnings(action='ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import math\n",
    "from torch.utils.data.sampler import Sampler\n",
    "import itertools\n",
    "import tqdm\n",
    "from torch.utils.data import TensorDataset\n",
    "#device = torch.device('cuda:7' if torch.cuda.is_available() else 'cpu')\n",
    "#print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clinical information embedding\n",
    "def clinic_concat(data,info):\n",
    "    for i in range(0,int(data.shape[1]/shell)+1):\n",
    "        index1 = shell*i\n",
    "        index2 = index1 + (shell -1)\n",
    "        if index2 > data.shape[1]:\n",
    "            index2 = data.shape[1]\n",
    "        tmp = data.loc[:,index1:index2]\n",
    "        tmp = pd.concat([info,tmp],axis=1)\n",
    "        if i==0:\n",
    "            data_concat = tmp\n",
    "        else:\n",
    "            data_concat = pd.concat([data_concat,tmp],axis=1)\n",
    "    return data_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_triplet(dat1,dat2,n):\n",
    "    triplet = []\n",
    "    # Anchor = group1 (Disease)\n",
    "    for x in range(int(n/2)):\n",
    "        anchor_index = x%dat1.shape[0]               \n",
    "        while True:\n",
    "            pos_index = random.randint(0,dat1.shape[0]-1)\n",
    "            if anchor_index != pos_index:\n",
    "                break\n",
    "        neg_index = random.randint(0,dat2.shape[0]-1)\n",
    "        triplet.append([ dat1[anchor_index,],dat1[pos_index,],dat2[neg_index,] ])\n",
    "    \n",
    "    # Anchor = group2 (Control)\n",
    "    for x in range(int(n/2)):\n",
    "        anchor_index = x%dat2.shape[0]               \n",
    "        while True:\n",
    "            pos_index = random.randint(0,dat2.shape[0]-1)\n",
    "            if anchor_index != pos_index:\n",
    "                break\n",
    "        neg_index = random.randint(0,dat1.shape[0]-1)\n",
    "        triplet.append([ dat2[anchor_index,],dat2[pos_index,],dat1[neg_index,] ])\n",
    "    \n",
    "    return triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.best_model = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_model = model\n",
    "            self.val_loss_min = val_loss\n",
    "        elif score > self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.val_loss_min = val_loss\n",
    "            self.best_model = model\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_1L(nn.Module):\n",
    "    def __init__(self,feature_dimension=100):\n",
    "        super(CNN_1L, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size= (shell+2), stride= (shell+2), padding=0)\n",
    "            #nn.BatchNorm1d(16)\n",
    "            \n",
    "            )\n",
    "        self.pool = nn.MaxPool1d(2,stride=2)\n",
    "        self.fc = nn.Linear(656, feature_dimension)\n",
    "        self.act = nn.Sigmoid()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # expected conv1d input : minibatch_size x num_channel x width\n",
    "        \n",
    "        x = x.view(x.shape[0], 1,-1)\n",
    "        out = self.conv(x)\n",
    "        out = self.act(out)\n",
    "        out = self.pool(out)\n",
    "        out = out.view(x.shape[0], out.size(1) * out.size(2))\n",
    "        logit = self.fc(out)\n",
    "        \n",
    "        return logit\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Triple_Distance(nn.Module):\n",
    "    def __init__(self, cosine = False):\n",
    "        super().__init__()\n",
    "        self.cosine = cosine\n",
    "        \n",
    "    def forward(self, anchor, positive, negative):\n",
    "        # Use Cosine Distance\n",
    "        if self.cosine:\n",
    "            self.margin = 1\n",
    "            # L2 Normalization\n",
    "            anchor_normed = F.normalize(anchor, p=2, dim=1)\n",
    "            positive_normed = F.normalize(positive, p=2, dim=1)\n",
    "            negative_normed = F.normalize(negative, p=2, dim=1)\n",
    "            \n",
    "            # Cosine Distance\n",
    "            distance_positive = torch.sum(torch.mul(anchor_normed, positive_normed), dim=1, keepdim=True)\n",
    "            distance_negative = torch.sum(torch.mul(anchor_normed, negative_normed), dim=1, keepdim=True)\n",
    "            \n",
    "        # Use Euclidian Distance\n",
    "        else:\n",
    "            distance_positive = (anchor - positive).pow(2).sum(1)  \n",
    "            distance_negative = (anchor - negative).pow(2).sum(1)  \n",
    "\n",
    "        return distance_positive, distance_negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mysampler(Sampler):\n",
    "    def __init__(self, dataset,index1):\n",
    "        self.index1 = index1\n",
    "        \n",
    "    def __iter__(self):\n",
    "        random.shuffle(self.index1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return iter(self.index1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return (len(self.index1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIntersection(a,b):\n",
    "    indices = torch.zeros_like(a, dtype=torch.uint8)\n",
    "    \n",
    "    for elem in b:\n",
    "        indices = indices| (a==elem).type(torch.uint8)\n",
    "\n",
    "    intersection = a[indices.type(torch.bool)]\n",
    "    return intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,patience,n_epochs, deg):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    avg_train_losses = []\n",
    "    avg_valid_losses = []\n",
    "    degree = deg\n",
    "\n",
    "    train_len = []\n",
    "    \n",
    "    distance = Triple_Distance()\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        total_pos = []\n",
    "        total_neg = []\n",
    "        for i,data in enumerate(train_dataloader):\n",
    "            \n",
    "            anchor, pos, neg = data\n",
    "            anchor, pos, neg = anchor.float().cuda(), pos.float().cuda(), neg.float().cuda()\n",
    "\n",
    "            \n",
    "            #########################################\n",
    "            # Loss pre-calculate for triplet mining #\n",
    "            #########################################\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Forward pass\n",
    "                output1 = model(anchor)\n",
    "                output2 = model(pos)\n",
    "                output3 = model(neg)\n",
    "\n",
    "            \n",
    "            \n",
    "                distance_pos, distance_neg = distance(output1,output2,output3)\n",
    "                distance_pos_s, distance_neg_s = distance(output2,output1,output3)\n",
    "\n",
    "                target = torch.FloatTensor(distance_pos.size()).fill_(-1).cuda()\n",
    "                target = Variable(target)\n",
    "                \n",
    "                distance_pos = distance_pos.squeeze()\n",
    "                distance_neg = distance_neg.squeeze()\n",
    "                distance_pos_s = distance_pos_s.squeeze()\n",
    "                distance_neg_s = distance_neg_s.squeeze()\n",
    "\n",
    "                if i==0:\n",
    "                    total_pos = distance_pos\n",
    "                    total_neg = distance_neg\n",
    "\n",
    "                    total_pos_s = distance_pos_s\n",
    "                    total_neg_s = distance_neg_s\n",
    "\n",
    "                else:\n",
    "                    total_pos = torch.cat([total_pos,distance_pos])\n",
    "                    total_neg = torch.cat([total_neg,distance_neg])\n",
    "        \n",
    "                    total_pos_s = torch.cat([total_pos_s, distance_pos_s])\n",
    "                    total_neg_s = torch.cat([total_neg_s, distance_neg_s])\n",
    "        index1_1 = (total_pos < total_neg).nonzero().flatten()\n",
    "        index1_2 = (total_neg < (total_pos + margin)).nonzero().flatten()\n",
    "        \n",
    "        index1 = getIntersection(index1_1, index1_2)\n",
    "\n",
    "        index1_1 = (total_pos_s < total_neg_s).nonzero().flatten()\n",
    "        index1_2 = (total_neg_s < (total_neg_s + margin)).nonzero().flatten()\n",
    "\n",
    "        index1_s = getIntersection(index1_1,index1_2)\n",
    "        index2 = ((total_pos+margin) < total_neg).nonzero().flatten()\n",
    "        index1 = getIntersection(index1,index1_s)\n",
    "        \n",
    "        \n",
    "        if len(index1) < 10:\n",
    "            break\n",
    "\n",
    "        # Sampler define\n",
    "        my_sampler = Mysampler(train_trip,index1)\n",
    "        my_loader = torch.utils.data.DataLoader(train_trip, sampler = my_sampler, batch_size = train_batch_size)\n",
    "        \n",
    "        for i,data in enumerate(my_loader):\n",
    "            anchor, pos, neg = data\n",
    "            anchor, pos, neg = anchor.float().cuda(), pos.float().cuda(), neg.float().cuda()\n",
    "            \n",
    "            # Forward pass\n",
    "            output1 = model(anchor)\n",
    "            output2 = model(pos)\n",
    "            output3 = model(neg)\n",
    "        \n",
    "            output_cen = torch.true_divide((output1+output2),2)\n",
    "            distance_pos, distance_neg = distance(output1, output2, output3)\n",
    "            distance_pos_s, distance_neg_s = distance(output2,output1,output3)\n",
    "            \n",
    "            distance_cen_pos,distance_cen_neg = distance(output_cen,output2,output3)\n",
    "            tana = math.tan(math.pi *(degree/180))\n",
    "            tmp = distance_pos -4*(tana**2)*distance_cen_neg\n",
    "            loss_ang = tmp[tmp>0].sum()\n",
    "            \n",
    "            target = torch.FloatTensor(distance_pos.size()).fill_(-1)\n",
    "            target = target.cuda()\n",
    "            target = Variable(target)\n",
    "            loss = criterion(distance_pos, distance_neg, target) + criterion(distance_pos_s,distance_neg_s,target) + loss_ang\n",
    "            \n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        with torch.no_grad():\n",
    "            model.eval() # prep model for evaluation\n",
    "            total_pos = []\n",
    "            total_neg = []\n",
    "            for i,data in enumerate(valid_dataloader):\n",
    "                anchor, pos, neg = data\n",
    "                anchor, pos, neg = anchor.float(), pos.float(), neg.float()\n",
    "                anchor, pos, neg = anchor.cuda(), pos.cuda(), neg.cuda()\n",
    "                \n",
    "                \n",
    "                ###################\n",
    "                # Loss pre-calculate\n",
    "                ###################\n",
    "                               \n",
    "                # Forward pass\n",
    "                output1 = model(anchor)\n",
    "                output2 = model(pos)\n",
    "                output3 = model(neg)\n",
    "                \n",
    "                distance_pos, distance_neg = distance(output1, output2, output3)\n",
    "                distance_pos_s, distance_neg_s = distance(output2,output1,output3)\n",
    "                \n",
    "                target = torch.FloatTensor(distance_pos.size()).fill_(-1)\n",
    "                target = target.cuda()\n",
    "                target = Variable(target)\n",
    "                \n",
    "                distance_pos = distance_pos.squeeze()\n",
    "                distance_neg = distance_neg.squeeze()\n",
    "                distance_pos_s = distance_pos_s.squeeze()\n",
    "                distance_neg_s = distance_neg_s.squeeze()\n",
    "\n",
    "\n",
    "                if i==0:\n",
    "                    total_pos = distance_pos\n",
    "                    total_neg = distance_neg\n",
    "\n",
    "                    total_pos_s = distance_pos_s\n",
    "                    total_neg_s = distance_neg_s\n",
    "                else:\n",
    "                    total_pos = torch.cat([total_pos,distance_pos])\n",
    "                    total_neg = torch.cat([total_neg,distance_neg])\n",
    "                    \n",
    "                    total_pos_s = torch.cat([total_pos_s,distance_pos_s])\n",
    "                    total_neg_s = torch.cat([total_neg_s, distance_neg_s])\n",
    "            \n",
    "            # Semi hard\n",
    "            index1_1 = (total_pos < total_neg).nonzero().flatten()\n",
    "            index1_2 = (total_neg < (total_pos + margin)).nonzero().flatten()\n",
    "        \n",
    "            index1 = getIntersection(index1_1, index1_2)\n",
    "\n",
    "            index1_1 = (total_pos_s <total_neg_s).nonzero().flatten()\n",
    "            index1_2 = (total_neg_s <(total_pos_s + margin)).nonzero().flatten()\n",
    "            \n",
    "            index1_s = getIntersection(index1_1,index1_2)\n",
    "            index1 = getIntersection(index1,index1_s)\n",
    "\n",
    "            my_sampler2 = Mysampler(valid_trip,index1)\n",
    "            \n",
    "            my_loader2 = torch.utils.data.DataLoader(valid_trip, sampler = my_sampler2, batch_size = valid_batch_size)\n",
    "            \n",
    "            \n",
    "            for i,data in enumerate(my_loader2):    \n",
    "                anchor, pos, neg = data\n",
    "                anchor, pos, neg = anchor.float(), pos.float(), neg.float()\n",
    "                anchor, pos, neg = anchor.cuda(), pos.cuda(), neg.cuda()\n",
    "                \n",
    "                # Forward pass\n",
    "                output1 = model(anchor)\n",
    "                output2 = model(pos)\n",
    "                output3 = model(neg)\n",
    "                \n",
    "                output_cen = torch.true_divide((output1+output2),2)\n",
    "\n",
    "                distance_pos, distance_neg = distance(output1, output2, output3)\n",
    "                distance_pos_s, distance_neg_s = distance(output2,output1,output3) \n",
    "                distance_cen_pos,distance_cen_neg = distance(output_cen,output2,output3)\n",
    "\n",
    "                target = torch.FloatTensor(distance_pos.size()).fill_(-1)\n",
    "                target = target.cuda()\n",
    "                target = Variable(target)\n",
    "                \n",
    "                tana = math.tan(math.pi*(degree/180))\n",
    "                tmp = distance_pos-4*(tana**2)*distance_cen_neg\n",
    "                loss_ang = tmp[tmp>0].sum()\n",
    "\n",
    "                loss = criterion(distance_pos, distance_neg, target) + criterion(distance_pos_s, distance_neg_s,target) + loss_ang\n",
    "                valid_losses.append(loss.item())\n",
    "                \n",
    "                \n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "        \n",
    "        epoch_len = len(str(n_epochs))\n",
    "        \n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss:.5f} ' +\n",
    "                     f'valid_loss: {valid_loss:.5f}')\n",
    "        # print(print_msg)\n",
    "       \n",
    "        # clear lists to track next epoch\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        \n",
    "        early_stopping(valid_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            break\n",
    "    \n",
    "    # load the last checkpoint with the best model\n",
    "    best_model = early_stopping.best_model\n",
    "    \n",
    "    return  best_model, early_stopping.best_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv1d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        torch.nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "shell = 200\n",
    "num_train_samples = 30000\n",
    "num_valid_samples = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_perform = 0\n",
    "margin = 0.5\n",
    "train_batch_size = 1000\n",
    "drop_prob = 0.0\n",
    "valid_batch_size = 1000\n",
    "rg = 0.0001\n",
    "degree_list = [45,60]\n",
    "feature_dimension_list = [60,90,150,300]\n",
    "learning_rate_list = [0.01,0.005]\n",
    "\n",
    "patience = 100\n",
    "n_epochs = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,feature_dimension):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        #self.act = nn.RReLU()\n",
    "        self.fc = nn.Linear(feature_dimension, 1) \n",
    "        #self.dropout = torch.nn.Dropout(p=drop_prob)\n",
    "               \n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.fc(x)\n",
    "        #out = self.dropout(out)\n",
    "        #out = self.act(out)\n",
    "        out = out.view(-1,)\n",
    "        \n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "x = pd.read_csv(\"./dat.csv\",header=None)\n",
    "y = pd.read_csv(\"./label.txt\",header=None)\n",
    "info = pd.read_csv(\"./dat_info.csv\",header=None)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "x_concat = clinic_concat(x,info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_index = []\n",
    "te_index = []\n",
    "for index1,index2 in skf.split(x_concat,y):\n",
    "    tr_index.append(index1)\n",
    "    te_index.append(index2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [8:39:58<00:00, 1949.92s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search finish\n",
      "1th Fold AUC Triplet embbedding(JTSC):0.886\n",
      "Fold:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [8:46:52<00:00, 1975.75s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search finish\n",
      "2th Fold AUC Triplet embbedding(JTSC):0.745\n",
      "Fold:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [7:16:10<00:00, 1635.69s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3th Fold AUC Triplet embbedding(JTSC):0.759\n",
      "Fold:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [6:59:18<00:00, 1572.39s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search finish\n",
      "4th Fold AUC Triplet embbedding(JTSC):0.850\n",
      "Fold:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [6:19:59<00:00, 1424.95s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search finish\n",
      "5th Fold AUC Triplet embbedding(JTSC):0.811\n",
      "---------------------------------------------------------\n",
      "Average Triplet embedding(JTSC) AUC:  0.8102632844738107\n"
     ]
    }
   ],
   "source": [
    "auc = []\n",
    "param_list = []\n",
    "for fold_ in range(1,6):\n",
    "    iter_auc = []\n",
    "    print(\"Fold: \", str(fold_))\n",
    "    \n",
    "    train_index = tr_index[fold_-1]\n",
    "    test_index = te_index[fold_-1]\n",
    "    valid_index = random.sample(list(train_index), int(0.2*len(train_index)))\n",
    "    train_index = np.setdiff1d(train_index,valid_index)\n",
    "    \n",
    "    train_x_concat = x_concat.loc[train_index,]\n",
    "    train_y = y.loc[train_index]\n",
    "    group_index = (train_y.squeeze()==0)\n",
    "    train_x1 = train_x_concat.loc[group_index,:].to_numpy()\n",
    "    train_x2 = train_x_concat.loc[-group_index,:].to_numpy()\n",
    "    \n",
    "    valid_x_concat = x_concat.loc[valid_index,]\n",
    "    valid_y = y.loc[valid_index]\n",
    "    group_index = (valid_y.squeeze()==0)\n",
    "    valid_x1 = valid_x_concat.loc[group_index,:].to_numpy()\n",
    "    valid_x2 = valid_x_concat.loc[-group_index,:].to_numpy()\n",
    "\n",
    "    test_x_concat = x_concat.loc[test_index,]\n",
    "    test_y = y.loc[test_index]\n",
    "    \n",
    "\n",
    "    # Create Triplets\n",
    "    train_trip = create_triplet(train_x1,train_x2,num_train_samples)\n",
    "    valid_trip = create_triplet(valid_x1,valid_x2,num_valid_samples)\n",
    "\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_trip, batch_size = 300,shuffle=False)\n",
    "    valid_dataloader = torch.utils.data.DataLoader(valid_trip, batch_size= 50, shuffle=False)\n",
    "\n",
    "    train_y = torch.FloatTensor(train_y.values.reshape(-1)).cuda()\n",
    "    valid_y = torch.FloatTensor(valid_y.values.reshape(-1)).cuda()\n",
    "    test_y = torch.FloatTensor(test_y.values.reshape(-1)).cuda()\n",
    "        \n",
    "        \n",
    "    # Search best parameter from validation set\n",
    "    val_set = list(itertools.product(degree_list,feature_dimension_list,learning_rate_list))\n",
    "    best_valid = 9999\n",
    "    \n",
    "    for iter_ in tqdm.tqdm(range(0,len(val_set))):\n",
    "        # print(str(x)+\"th Searching\")\n",
    "        val_par = val_set[iter_]\n",
    "        \n",
    "        net_ = CNN_1L(val_par[1]).cuda()\n",
    "        net_.apply(weights_init)\n",
    "        criterion = torch.nn.MarginRankingLoss(margin=margin)\n",
    "        optimizer = optim.Adam(net_.parameters(),lr = val_par[2])\n",
    "        _, best_score = train_model(net_, patience, n_epochs, val_par[0])\n",
    "            \n",
    "        if best_score < best_valid:\n",
    "            best_valid = best_score\n",
    "            save_degree = val_par[0]\n",
    "            save_feature_dimension = val_par[1]\n",
    "            save_learning_rate = val_par[2]\n",
    "    param_list.append([save_degree,save_feature_dimension,save_learning_rate])        \n",
    "    print(\"Search finish\")\n",
    "    \n",
    "    for iter_ in range(0,3):\n",
    "        # Train Embedding function\n",
    "        net = CNN_1L(save_feature_dimension).cuda()\n",
    "        net.apply(weights_init)\n",
    "        criterion = torch.nn.MarginRankingLoss(margin= margin)\n",
    "\n",
    "        optimizer = optim.Adam(net.parameters(),lr = save_learning_rate)\n",
    "\n",
    "\n",
    "        net, _ = train_model(net,patience,n_epochs, save_degree)\n",
    "\n",
    "        train_x = net(torch.tensor(train_x_concat.values).float().cuda()).detach()\n",
    "        valid_x = net(torch.tensor(valid_x_concat.values).float().cuda()).detach()\n",
    "        test_x = net(torch.tensor(test_x_concat.values).float().cuda()).detach()\n",
    "\n",
    "        clf = Net(save_feature_dimension).cuda()\n",
    "        clf.apply(weights_init)\n",
    "        optimizer = torch.optim.Adam(clf.parameters(), lr = 0.005)\n",
    "        loss_func = nn.BCEWithLogitsLoss(weight = None, size_average=None,reduce=None,reduction='mean')\n",
    "\n",
    "        emb_train_loader = torch.utils.data.DataLoader(TensorDataset(train_x,train_y), batch_size=50)\n",
    "        emb_valid_loader = torch.utils.data.DataLoader(TensorDataset(valid_x,valid_y), batch_size=30)\n",
    "\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        early_stopping = EarlyStopping(patience = patience, verbose = False)\n",
    "        clf.train()\n",
    "\n",
    "        for epoch in range(1,n_epochs+1):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            for i,data in enumerate(emb_train_loader):\n",
    "                x_ , y_ = data\n",
    "                output = clf(x_)\n",
    "\n",
    "                loss = loss_func(output,y_)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_losses.append(loss.item())\n",
    "\n",
    "            with torch.no_grad():\n",
    "                clf.eval()\n",
    "                for i,data in enumerate(emb_valid_loader):\n",
    "                    x_ , y_ = data\n",
    "                    output = clf(x_)\n",
    "                    loss = loss_func(output,y_)\n",
    "\n",
    "                    valid_losses.append(loss.item())\n",
    "\n",
    "            train_loss = np.average(train_losses)\n",
    "            valid_loss = np.average(valid_losses)\n",
    "\n",
    "            epoch_len = len(str(n_epochs))\n",
    "\n",
    "            print_msg = (f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +\n",
    "                                    f'train_loss: {train_loss:.5f} ' +\n",
    "                                    f'valid_loss: {valid_loss:.5f}')\n",
    "\n",
    "            #print(print_msg)\n",
    "            train_losses = []\n",
    "            valid_losses = []\n",
    "            early_stopping(valid_loss, clf)\n",
    "\n",
    "            if early_stopping.early_stop:\n",
    "                break\n",
    "\n",
    "        # load the last checkpoint with the best model\n",
    "        clf = early_stopping.best_model\n",
    "        pred = torch.sigmoid(clf(test_x))\n",
    "        pred = pred.data\n",
    "        pred = pred.cpu().numpy()\n",
    "        roc_auc = roc_auc_score(test_y.cpu().numpy(),pred)\n",
    "        iter_auc.append(roc_auc)\n",
    "        #print(roc_auc)\n",
    "\n",
    "    print(str(fold_)+f\"th Fold AUC Triplet embbedding(JTSC):{sum(iter_auc)/len(iter_auc):.3f}\")\n",
    "    auc.append( sum(iter_auc)/len(iter_auc) )\n",
    "    \n",
    "\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(\"Average Triplet embedding(JTSC) AUC: \", sum(auc)/len(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold1:  [degree 45 , embedding dimension 150 ,learning_rate 0.01 ]\n",
      "Fold2:  [degree 45 , embedding dimension 90 ,learning_rate 0.005 ]\n",
      "Fold3:  [degree 60 , embedding dimension 300 ,learning_rate 0.005 ]\n",
      "Fold4:  [degree 60 , embedding dimension 150 ,learning_rate 0.01 ]\n",
      "Fold5:  [degree 45 , embedding dimension 300 ,learning_rate 0.01 ]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,(len(param_list)+1)):\n",
    "    print(\"Fold\"+str(i)+\": \",\"[degree\",param_list[i-1][0],\", embedding dimension\",param_list[i-1][1],\",learning_rate\",param_list[i-1][2],\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_svm(train_x,train_y,valid_x,valid_y,test_x,test_y):\n",
    "    p1 = [\"linear\",\"poly\",\"rbf\",\"sigmoid\"] #kernel\n",
    "    p2 = ['auto','scale'] #gamma\n",
    "    p3 = [0.001,0.01,0.005] #tol\n",
    "    best = 0.0\n",
    "    val_set = list(itertools.product(p1,p2,p3))\n",
    "    for i in range(0,len(val_set)):\n",
    "        val_par = val_set[i]\n",
    "        clf = svm.SVC(kernel=val_par[0], gamma=val_par[1], tol=val_par[2])\n",
    "        \n",
    "        clf.fit(train_x,train_y)\n",
    "        y_pred = clf.predict(valid_x)\n",
    "        \n",
    "        roc_auc = roc_auc_score(valid_y,y_pred)\n",
    "        if roc_auc > best:\n",
    "            best = roc_auc\n",
    "            y_pred_ = clf.predict(test_x)\n",
    "            res = roc_auc_score(test_y,y_pred_)\n",
    "                    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1 test AUC: 0.6586956521739131\n",
      "Fold: 2 test AUC: 0.6341614906832299\n",
      "Fold: 3 test AUC: 0.7524844720496894\n",
      "Fold: 4 test AUC: 0.6770963704630788\n",
      "Fold: 5 test AUC: 0.6240409207161125\n",
      "SVM Average AUC :  0.6692957812172048\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "fold = 5\n",
    "for fold_ in range(1,fold+1):\n",
    "    \n",
    "    train_index = tr_index[fold_-1]\n",
    "    test_index = te_index[fold_-1]\n",
    "    valid_index = random.sample(list(train_index), int(0.2*len(train_index)))\n",
    "    train_index = np.setdiff1d(train_index,valid_index)\n",
    "    \n",
    "    train_x_concat = pd.concat([x,info],axis=1).loc[train_index,]\n",
    "    train_y = y.loc[train_index]\n",
    "    \n",
    "    valid_x_concat = pd.concat([x,info],axis=1).loc[valid_index,]\n",
    "    valid_y = y.loc[valid_index]\n",
    "    \n",
    "    test_x_concat = pd.concat([x,info],axis=1).loc[test_index,]\n",
    "    test_y = y.loc[test_index]\n",
    "        \n",
    "    x_train = train_x_concat.to_numpy()\n",
    "    x_valid = valid_x_concat.to_numpy()\n",
    "    x_test = test_x_concat.to_numpy()\n",
    "        \n",
    "    y_train = train_y.values.reshape(-1)\n",
    "    y_valid = valid_y.values.reshape(-1)\n",
    "    y_test = test_y.values.reshape(-1)\n",
    "        \n",
    "               \n",
    "        \n",
    "    auc = cross_svm(x_train,y_train,x_valid,y_valid,x_test,y_test)\n",
    "    res.append(auc)\n",
    "    print(\"Fold:\",str(fold_),\"test AUC:\",auc)\n",
    "\n",
    "print(\"SVM Average AUC : \" ,sum(res)/len(res))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
